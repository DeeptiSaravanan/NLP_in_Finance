{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SAT_Order_Final_Stats.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLU3xqu7QAjF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "3297761c-0e5c-4b24-af29-40005d1b0e35"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "#import en_core_web_lg\n",
        "import en_core_web_sm\n",
        "!pip install tika\n",
        "!pip install xlwt"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tika in /usr/local/lib/python3.6/dist-packages (1.24)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from tika) (49.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from tika) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->tika) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->tika) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->tika) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->tika) (1.24.3)\n",
            "Requirement already satisfied: xlwt in /usr/local/lib/python3.6/dist-packages (1.3.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jfcMqIGQIW_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tika import parser\n",
        "from xlwt import Workbook "
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Dl87_ISQKSM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = en_core_web_sm.load()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTyw9d28Sa7g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_file(file_path):\n",
        "  raw = parser.from_file(file_path)\n",
        "  return raw['content']\n",
        "\n",
        "def preprocess(pt):\n",
        "  pt = re.sub('\\[.*\\]',' ',pt)\n",
        "  pt = re.sub('\\(.*\\)',' ',pt)\n",
        "  pt = re.sub('\\n',' ',pt)\n",
        "  pt = re.sub('\\n\\n',' ',pt)\n",
        "  pt = re.sub('Order\\%.*\\%.',\"\",pt)\n",
        "  pt = re.sub('Microsoft Word',\"\",pt)  \n",
        "  pt = re.sub('\\sPage\\s[0-9]\\sof\\s[0-9]?[0-9]\\s','',pt)\n",
        "  pt = re.sub('.*docx','',pt)\n",
        "  #pt = re.sub('_*_','',pt)\n",
        "  pt = re.sub('/',' ',pt)\n",
        "  pt = re.sub('`',' ',pt)\n",
        "  pt = re.sub('\\sPage\\s[0-9]','',pt)\n",
        "  #pt = re.sub('[0-9]?[0-9]\\.','',pt)\n",
        "  pt = pt.lstrip()\n",
        "  pt = pt.rstrip()\n",
        "  return pt\n",
        "\n",
        "def split_header(text):\n",
        "  appellants_list = []\n",
        "  all_names = []\n",
        "  bench = []\n",
        "  \n",
        "  if text.find('Per :')>-1:\n",
        "    header_extracted = text.split('Per :')[0]\n",
        "    body_sep = text.split('Per :')[1]\n",
        "  elif text.find('Per:')>-1:\n",
        "    header_extracted = text.split('Per:')[0]\n",
        "    body_sep = text.split('Per:')[1]\n",
        "  elif text.find('Presiding Officer')>-1:\n",
        "    header_extracted = text.split('Presiding Officer')[0]\n",
        "    body_sep = text.split('Presiding Officer')[1]\n",
        "  elif text.find('ORDER')>-1:\n",
        "    header_extracted = text.split('ORDER')[0]\n",
        "    body_sep = text.split('ORDER')[1]\n",
        "  elif text.find('JUDGMENT')>-1:\n",
        "    header_extracted = text.split('JUDGMENT')[0]\n",
        "    body_sep = text.split('JUDGMENT')[1]\n",
        "  else:\n",
        "    print(\"WARNING: Could not split header correctly\")\n",
        "\n",
        "  return header_extracted, body_sep\n",
        "\n",
        "def header_extraction(header):\n",
        "  #store names of appellants \n",
        "  appellant_names = []\n",
        "  #all the names in the header \n",
        "  all_header_names  = []\n",
        "  #names of the bench \n",
        "  bench_names = []\n",
        "  #names of organization in the header \n",
        "  org_names = []\n",
        "\n",
        "  #run NER on entire header to detect all possible names \n",
        "  preprocessed_header = preprocess(header)\n",
        "  doc = nlp(preprocessed_header)\n",
        "  for ent in doc.ents:\n",
        "    if ent.label_ == \"PERSON\":\n",
        "      all_header_names.append(ent.text)\n",
        "\n",
        "  if 'versus' in header.lower():\n",
        "    #split into appellants\n",
        "    appellant_split = header.split('Versus')\n",
        "    if len(appellant_split)>2:\n",
        "      print('Appellant Exception ',end='\\n\\n\\n')\n",
        "    # Check if there are multiple appellants named in the header\n",
        "    if 'appellants' in appellant_split[0].lower():\n",
        "      #split into points \n",
        "      print(\"Multiple Appellants\",end='\\n\\n\\n')\n",
        "    else: \n",
        "      appellant_segment_preprocess = preprocess(appellant_split[0])\n",
        "      doc = nlp(appellant_segment_preprocess)\n",
        "      for ent in doc.ents:\n",
        "        if ent.label_ == \"PERSON\":\n",
        "          appellant_names.append(ent.text)\n",
        "        if ent.label_ == \"ORG\":\n",
        "          org_names.append(ent.text)\n",
        "      #get names of the bench \n",
        "    match = re.search('coram',header,re.I)\n",
        "    occurance  = match.group()\n",
        "    coram_fragment = header.split(occurance)[1]\n",
        "    doc = nlp(preprocess(coram_fragment))\n",
        "    for ent in doc.ents:\n",
        "      if ent.label_ == \"PERSON\":\n",
        "        bench_names.append(ent.text)\n",
        "  else:\n",
        "    #template for type 2 documents \n",
        "    #get names of judges \n",
        "    match = re.search('bench',header,re.I)\n",
        "    occurance  = match.group()\n",
        "    coram_fragment = header.split(occurance)[1]\n",
        "    doc = nlp(preprocess(coram_fragment))\n",
        "    for ent in doc.ents:\n",
        "      if ent.label_ == \"PERSON\":\n",
        "        bench_names.append(ent.text)\n",
        "    # get ORG names \n",
        "    preprocessed_header = preprocess(header)\n",
        "    doc = nlp(preprocessed_header)\n",
        "    for ent in doc.ents:\n",
        "      if ent.label_ == \"ORG\":\n",
        "        org_names.append(ent.text)\n",
        "    #names of appellant \n",
        "    appellant_names = [name for name in all_names if name not in bench_names]\n",
        "\n",
        "\n",
        "  return all_header_names,bench_names,appellant_names,org_names\n",
        "\n",
        "def split_pointwise(text):\n",
        "  points_split = re.split(\"\\n[0-9]?[0-9]\\.\\s\",text)\n",
        "  total_points = len(points_split)\n",
        "  return points_split,total_points\n",
        "\n",
        "\n",
        "def extract_all_entites(points):\n",
        "  all_DATE = []\n",
        "  all_GPE = []\n",
        "  all_LAW = []\n",
        "  all_MONEY = []\n",
        "  all_ORG = []\n",
        "  all_PER = []\n",
        "  all_CARDINAL = []\n",
        "\n",
        "  for point in points[1:]:\n",
        "    point = preprocess(point)\n",
        "    doc = nlp(point)\n",
        "    for ent in doc.ents:\n",
        "      if ent.label_ == \"PERSON\":\n",
        "        all_PER.append(ent.text)\n",
        "      if ent.label_ == \"ORG\":\n",
        "        all_ORG.append(ent.text)\n",
        "      if ent.label_ == \"DATE\":\n",
        "        all_DATE.append(ent.text)\n",
        "      if ent.label_ == \"LAW\":\n",
        "        all_LAW.append(ent.text)\n",
        "      if ent.label_ == \"MONEY\":\n",
        "        all_MONEY.append(ent.text)\n",
        "      if ent.label_ == \"GPE\":\n",
        "        all_GPE.append(ent.text)\n",
        "      if ent.label_ == \"CARDINAL\":\n",
        "        all_CARDINAL.append(ent.text)\n",
        "\n",
        "  return all_DATE,all_GPE,all_LAW,all_MONEY,all_ORG,all_PER,all_CARDINAL\n",
        "#Rule based extraction of penalty and money using spacy matcher and regex \n",
        "\n",
        "def rule_based_matcher_penalty(points):\n",
        "\n",
        "  pattern_one = [{'IS_ALPHA': False},\n",
        "           {'ENT_TYPE': 'CARDINAL'},\n",
        "           {'LOWER': 'lacs'}]\n",
        "  pattern_two = [{'IS_ALPHA': False},\n",
        "            {'ENT_TYPE': 'CARDINAL'},\n",
        "            {'LOWER': 'lakh'}]\n",
        "  pattern_three = [{'IS_ALPHA': False},\n",
        "            {'ENT_TYPE': 'CARDINAL'},\n",
        "            {'LOWER': 'lakhs'}]\n",
        "\n",
        "  pattern_four = [{'IS_ALPHA': False},\n",
        "            {'ENT_TYPE': 'CARDINAL'},\n",
        "            {'LOWER': 'crore'}]\n",
        "\n",
        "  pattern_five = [{'IS_ALPHA': False},\n",
        "            {'ENT_TYPE': 'CARDINAL'},\n",
        "            {'LOWER': 'crores'}]\n",
        "  pattern_six = [{'LOWER': 'rs'},\n",
        "            {'IS_PUNCT': True, 'OP': '?'},\n",
        "            {'ENT_TYPE': 'CARDINAL'},\n",
        "            {'LOWER': 'lacs'}]\n",
        "  pattern_seven = [{'LOWER': 'rs'},\n",
        "            {'IS_PUNCT': True, 'OP': '?'},\n",
        "            {'ENT_TYPE': 'CARDINAL'},\n",
        "            {'LOWER': 'lakhs'}]\n",
        "  pattern_eight = [{'LOWER': 'rs'},\n",
        "            {'IS_PUNCT': True, 'OP': '?'},\n",
        "            {'ENT_TYPE': 'CARDINAL'},\n",
        "            {'LOWER': 'crore'}]               \n",
        "  pattern_nine = [{'LOWER': 'rs'},\n",
        "            {'IS_PUNCT': True, 'OP': '?'},\n",
        "            {'ENT_TYPE': 'CARDINAL'},\n",
        "            {'LOWER': 'crores'}]\n",
        "\n",
        "\n",
        "  matcher = Matcher(nlp.vocab)\n",
        "  matcher.add(\"Rule1\",None,pattern_one)\n",
        "  matcher.add(\"Rule2\",None,pattern_two)\n",
        "  matcher.add(\"Rule3\",None,pattern_three)\n",
        "  matcher.add(\"Rule4\",None,pattern_four)\n",
        "  matcher.add(\"Rule5\",None,pattern_five)\n",
        "  matcher.add(\"Rule6\",None,pattern_six)\n",
        "  matcher.add(\"Rule7\",None,pattern_seven)\n",
        "  matcher.add(\"Rule8\",None,pattern_eight)\n",
        "  matcher.add(\"Rule9\",None,pattern_nine)\n",
        "\n",
        "  all_penalty = []\n",
        "  all_money = []\n",
        "\n",
        "  for point in points[1:]:\n",
        "    preprocessed_point = preprocess(point)\n",
        "    doc = nlp(preprocessed_point)\n",
        "    tokens = [token.text for token in doc]\n",
        "    matches = matcher(doc)\n",
        "    if len(matches)>0:\n",
        "      for match_id, start, end in matches:\n",
        "        if point.lower().find('penalty') >-1:\n",
        "          all_penalty.append(\" \".join(tokens[start+1:end]))\n",
        "          #print(\" \".join(tokens[start+1:end]))\n",
        "        else:\n",
        "          all_money.append(\" \".join(tokens[start+1:end]))\n",
        "          #print(\" \".join(tokens[start+1:end]))    \n",
        "  \n",
        "  return all_penalty,all_money\n",
        "\n",
        "def rule_based_regex_penalty(points):\n",
        "  all_penalty = []\n",
        "  all_money = []\n",
        "\n",
        "  for point in points[1:]:\n",
        "    preprocessed_point = preprocess(point)\n",
        "    indian_curr = re.compile(r'(Rs\\.?|rs.)\\s*(\\d+(?:[.,]\\d+)*)\\s*(lacs|lakhs|crore|crores)')\n",
        "    if point.lower().find('penalty') >-1:\n",
        "      pt_penalty = [y for y in indian_curr.findall(preprocessed_point)]\n",
        "      all_penalty += pt_penalty   \n",
        "    else:\n",
        "      pt_money = [y for y in indian_curr.findall(preprocessed_point)]\n",
        "      all_money += pt_money   \n",
        "\n",
        "  return all_penalty,all_money\n",
        "\n",
        "def decision(points):\n",
        "  preprocessed_point =  preprocess(points[-1])\n",
        "  match = re.search('sd -',preprocessed_point,re.I)\n",
        "  if match:\n",
        "    occurance = match.group()\n",
        "    preprocessed_point = preprocessed_point.split(occurance)[0]\n",
        "  \n",
        "  return preprocessed_point       \n",
        "     "
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_i38SxdtLV2M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Format excel sheet \n",
        "wb = Workbook()\n",
        "sheet1 = wb.add_sheet('Sheet 1')\n",
        "\n",
        "sheet1.write(0,0,'File Name')\n",
        "sheet1.write(0,1,'Header Names S-NER')\n",
        "sheet1.write(0,2,'Header Bench S-NER')\n",
        "sheet1.write(0,3,'Header Appellant S-NER')\n",
        "sheet1.write(0,4,'Header Orgs S-NER')\n",
        "sheet1.write(0,5,'Dates S-NER')\n",
        "sheet1.write(0,6,'GPE S-NER')\n",
        "sheet1.write(0,7,'Laws S-NER')\n",
        "sheet1.write(0,8,'Money S-NER')\n",
        "sheet1.write(0,9,'Orgs S-NER')\n",
        "sheet1.write(0,10,'People S-NER')\n",
        "sheet1.write(0,11,'Cardinals S-NER')\n",
        "sheet1.write(0,12,'Penalty SM')\n",
        "sheet1.write(0,13,'Money SM')\n",
        "sheet1.write(0,14,'Penalty Regex')\n",
        "sheet1.write(0,15,'Money Regex')\n",
        "#writing the decision to the excel file exceeds the allowed character count per cell \n",
        "#sheet1.write(0,16,'Decision')\n",
        "\n"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjvKGyUbQMi1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list_of_files = []\n",
        "root_name = ''\n",
        "file_count = 2\n",
        "\n",
        "for root,dir,files in os.walk('/content/drive/My Drive/SEBI /SAT orders/IndianKanoon/'):\n",
        "  list_of_files = files\n",
        "  root_name = root\n",
        "\n",
        "for file_name in list_of_files:\n",
        "  file_path = root_name+file_name\n",
        "  text = parse_file(file_path) \n",
        "  #split docuemnt from header \n",
        "  header, rest = split_header(text)\n",
        "  #print(header)\n",
        "  #header extraction\n",
        "  all_names, bench_names, appellant_names, org_names = header_extraction(header)\n",
        "  #print(all_names,bench_names,appellant_names,org_names,sep='\\n')\n",
        "  #split the remaining document into points \n",
        "  points, no_of_points = split_pointwise(rest)\n",
        "  #pass the points to get all the different types of entites in the doc \n",
        "  all_DATE,all_GPE,all_LAW,all_MONEY,all_ORG,all_PER,all_CARDINAL = extract_all_entites(points)\n",
        "  #print(all_DATE,all_GPE,all_LAW,all_MONEY,all_ORG,all_PER,all_CARDINAL,sep='\\n')\n",
        "  penalty_extracted_matcher, money_extracted_matcher = rule_based_matcher_penalty(points)\n",
        "  #print(penalty_extracted_matcher)\n",
        "  #print(money_extracted_matcher)\n",
        "  penalty_extracted_regex, money_extracted_regex = rule_based_regex_penalty(points)\n",
        "  #print(penalty_extracted_regex)\n",
        "  #print(money_extracted_regex)\n",
        "  verdict = decision(points)\n",
        "  #print(verdict)\n",
        "\n",
        "  ##Write the resuls to an output file \n",
        "  sheet1.write(file_count,0,file_name)\n",
        "  sheet1.write(file_count,1, ','.join(all_names))\n",
        "  sheet1.write(file_count,2,','.join(bench_names))\n",
        "  sheet1.write(file_count,3,','.join(appellant_names))\n",
        "  sheet1.write(file_count,4,','.join(org_names))\n",
        "  sheet1.write(file_count,5,','.join(all_DATE))\n",
        "  sheet1.write(file_count,6,','.join(all_GPE))\n",
        "  sheet1.write(file_count,7,','.join(all_LAW))\n",
        "  sheet1.write(file_count,8,','.join(all_MONEY))\n",
        "  sheet1.write(file_count,9,','.join(all_ORG))\n",
        "  sheet1.write(file_count,10,','.join(all_PER))\n",
        "  sheet1.write(file_count,11,','.join(all_CARDINAL))\n",
        "  sheet1.write(file_count,12,','.join(penalty_extracted_matcher))\n",
        "  sheet1.write(file_count,13,','.join(money_extracted_matcher))\n",
        "  sheet1.write(file_count,14,str(penalty_extracted_regex))\n",
        "  sheet1.write(file_count,15,str(money_extracted_regex))\n",
        "  #sheet1.write(file_count,16,verdict)  \n",
        "  file_count = file_count + 1 \n",
        "  print('Done',end='\\n')\n",
        "wb.save(\"SAT_Order_First_Run.xls\")"
      ],
      "execution_count": 61,
      "outputs": []
    }
  ]
}