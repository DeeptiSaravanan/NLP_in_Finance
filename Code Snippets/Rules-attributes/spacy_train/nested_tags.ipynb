{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import json\n",
    "import pickle\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "f = open('data_rules/data/1571225904231.json')\n",
    "object_json2 = json.load(f)\n",
    "# object_json2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tags(text, tag, rule):\n",
    "    start_tag = '<'+tag+'>'\n",
    "    end_tag = '</'+tag+'>'\n",
    "    n = (start_tag+text).join(rule.split(text))\n",
    "#     print(n)\n",
    "    #after\n",
    "    ch_rule = (text+end_tag).join(n.split(text))\n",
    "#     print(ch_rule)\n",
    "    return ch_rule\n",
    "#     idx = my_str.index(text)\n",
    "#     my_str = my_str[:idx] + inserttxt + my_str[idx:]\n",
    "#     new_text = rule[:start] + start_tag+rule[start:end]+ end_tag + rule[end:]\n",
    "#     return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "def adding_xml_tags(text, tag, rule):\n",
    "#     s = '<b>'+ rule+'</b>'\n",
    "#     print(s)\n",
    "    soup = BeautifulSoup(s, 'lxml')\n",
    "#     print(\"Original Markup:\")\n",
    "#     print(soup.b)\n",
    "    tag = soup.new_tag(tag)\n",
    "    tag.string = text\n",
    "#     print(\"\\nNew Markup, after inserting the text:\")\n",
    "    soup.b.string.insert_after(tag)\n",
    "#     k = str(soup.b)\n",
    "    return str(soup.b)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import spacy\n",
    "# rule_tag_dict = {}\n",
    "# rule_without_pos = {}\n",
    "# #to store tags rule wise.\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# output_dir = 'best_trained/'\n",
    "# nlp2 = spacy.load(output_dir)\n",
    "# ent_list = []\n",
    "# f = open('data_rules/data/1571225904231.json')\n",
    "# object_json2 = json.load(f)\n",
    "# for rule in object_json2:\n",
    "#     mod_text = add_tags(rule['text'],rule['tags']['tag_name'], rule['rule'])\n",
    "# #     print((rule['rule'][rule['span']['start']:rule['span']['end']], rule['text']))\n",
    "# #     ent_list.append({'tag_name':rule['tags']['tag_name'], 'start':rule['span']['start'], 'end':rule['span']['end'], 'text':rule['text']})\n",
    "# #     mod_text = adding_xml_tags(rule['text'],rule['tags']['tag_name'], '<b>'+rule['rule']+'</b' )\n",
    "#     doc2 = nlp2(rule['rule'])\n",
    "#     for ent in doc2.ents:\n",
    "# #         print(mod_text)\n",
    "#         mod_text = add_tags(ent.text,  ent.label_, mod_text)\n",
    "#     doc = nlp(rule['rule'])\n",
    "# #     print(rule['rule'])\n",
    "#     for token in doc:\n",
    "#         if token.pos_ in [\"VERB\", \"ADJ\"]:\n",
    "# #             print(mod_text)\n",
    "#             mod_text = add_tags(token.text,  token.pos_, mod_text)\n",
    "        \n",
    "# #         ent_list.append({'tag_name':token.pos_, 'start':token.idx, 'end': token.idx+len(token.text), 'text':token.text})\n",
    "#     #sorting spans.\n",
    "# #     ent_list = sorted(ent_list,\n",
    "# #                           key=lambda k: (k['start']))\n",
    "#     rule_tag_dict[rule['rule']] = mod_text\n",
    "# #     break\n",
    "# # rule_tag_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('single_file_nested_tags.json', 'w') as fp:\n",
    "    json.dump(rule_tag_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rule_tag_dict.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open('insider_nested_tags_indictformat.pkl','rb')\n",
    "rule_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import spacy\n",
    "# rule_tag_dict = {}\n",
    "# rule_without_pos = {}\n",
    "# #to store tags rule wise.\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# output_dir = 'best_trained/'\n",
    "# nlp2 = spacy.load(output_dir)\n",
    "# ent_list = []\n",
    "# f = open('data_rules/data/1571225904231.json')\n",
    "# object_json2 = json.load(f)\n",
    "# for rule in object_json2:\n",
    "# #     mod_text = add_tags(rule['text'],rule['tags']['tag_name'], rule['rule'])\n",
    "# #     print((rule['rule'][rule['span']['start']:rule['span']['end']], rule['text']))\n",
    "#     ent_list.append({'tag_name':rule['tags']['tag_name'], 'start':rule['span']['start'], 'end':rule['span']['end'], 'text':rule['text']})\n",
    "# #     mod_text = adding_xml_tags(rule['text'],rule['tags']['tag_name'], '<b>'+rule['rule']+'</b' )\n",
    "#     doc2 = nlp2(rule['rule'])\n",
    "#     for ent in doc2.ents:\n",
    "# #         print(mod_text)\n",
    "# #         mod_text = add_tags(ent.text,  ent.label_, mod_text)\n",
    "#         ent_list.append({'tag_name':ent.label_, 'start':ent.start, 'end':ent.end, 'text':ent.text})\n",
    "        \n",
    "#     doc = nlp(rule['rule'])\n",
    "# #     print(rule['rule'])\n",
    "#     for token in doc:\n",
    "#         if token.pos_ in [\"VERB\"]:\n",
    "# #             print(mod_text)\n",
    "# #             mod_text = add_tags(token.text,  token.pos_, mod_text)\n",
    "# #             ent_list.append({'tag_name':token.pos_, 'start':token.start, 'end':token.end, 'text':token.text})\n",
    "# #             pass\n",
    "#             ent_list.append({'tag_name':token.pos_, 'start':token.idx, 'end': token.idx+len(token.text), 'text':token.text})\n",
    "#     #sorting spans.\n",
    "# #     ent_list = sorted(ent_list,\n",
    "# #                           key=lambda k: (k['start']))\n",
    "#     rule_tag_dict[rule['rule']] = ent_list\n",
    "#     ent_list = []\n",
    "# #     break\n",
    "# rule_tag_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import spacy\n",
    "# rule_tag_dict_jj = {}\n",
    "# rule_without_pos = {}\n",
    "# #to store tags rule wise.\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# output_dir = 'my_mixed_random/'\n",
    "# nlp2 = spacy.load(output_dir)\n",
    "# ent_list = []\n",
    "# f = open('data_rules/data/1571225904231.json')\n",
    "# object_json2 = json.load(f)\n",
    "# for rule in object_json2:\n",
    "# #     mod_text = add_tags(rule['text'],rule['tags']['tag_name'], rule['rule'])\n",
    "# #     print((rule['rule'][rule['span']['start']:rule['span']['end']], rule['text']))\n",
    "#     ent_list.append({'tag_name':rule['tags']['tag_name'], 'start':rule['span']['start'], 'end':rule['span']['end'], 'text':rule['text']})\n",
    "# #     mod_text = adding_xml_tags(rule['text'],rule['tags']['tag_name'], '<b>'+rule['rule']+'</b' )\n",
    "#     doc2 = nlp2(rule['rule'])\n",
    "#     for ent in doc2.ents:\n",
    "# #         print(mod_text)\n",
    "# #         mod_text = add_tags(ent.text,  ent.label_, mod_text)\n",
    "#         ent_list.append({'tag_name':ent.label_, 'start':ent.start, 'end':ent.end, 'text':ent.text})\n",
    "        \n",
    "#     doc = nlp(rule['rule'])\n",
    "# #     print(rule['rule'])\n",
    "#     for token in doc:\n",
    "#         if token.pos_ in [\"VERB\"]:\n",
    "# #             print(mod_text)\n",
    "# #             mod_text = add_tags(token.text,  token.pos_, mod_text)\n",
    "# #             ent_list.append({'tag_name':token.pos_, 'start':token.start, 'end':token.end, 'text':token.text})\n",
    "# #             pass\n",
    "#             ent_list.append({'tag_name':token.pos_, 'start':token.idx, 'end': token.idx+len(token.text), 'text':token.text})\n",
    "#     #sorting spans.\n",
    "# #     ent_list = sorted(ent_list,\n",
    "# #                           key=lambda k: (k['start']))\n",
    "#     rule_tag_dict_jj[rule['rule']] = ent_list\n",
    "#     ent_list = []\n",
    "# #     break\n",
    "# rule_tag_dict_jj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collecting_common  = []\n",
    "# for k in rule_tag_dict_jj:\n",
    "#     d = {}\n",
    "#     f = {}\n",
    "#     for m in rule_tag_dict_jj[k]:\n",
    "#         try:\n",
    "# #             print(m['start'])\n",
    "# #             print(type(m['start']))\n",
    "#             u = (m['start'])\n",
    "#             if u not in d.keys():\n",
    "#                 d[u] = [m['tag_name']]\n",
    "#             else:\n",
    "#                 d[u].append(m['tag_name'])\n",
    "#         except:\n",
    "#             pass\n",
    "#     for key in d:\n",
    "#         if len(d[key])>1:\n",
    "#             f[key] = ' '.join(d[key])\n",
    "#         else:\n",
    "#             f[key] = d[key][0]\n",
    "        \n",
    "#     collecting_common.append((k,f))\n",
    "# #     new['rule'] = k\n",
    "# #     break\n",
    "# collecting_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag_collection_by_rule = []\n",
    "# for rule in collecting_common:\n",
    "#     sorted_dict = dict(sorted(rule[1].items()))\n",
    "#     l = list(sorted_dict.values())\n",
    "# #     flat_list = [item for sublist in l for item in sublist]\n",
    "#     tag_collection_by_rule.append(l)\n",
    "# tag_collection_by_rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def get_seq(x, M):\n",
    "    index_for_length_m = M - 1\n",
    "    for v in [l for l in x if len(l) >= M]:\n",
    "        for i in [i for i, v in enumerate(v[index_for_length_m:], start=index_for_length_m)]:\n",
    "            # convert to str to be hashable\n",
    "            yield str(v[i - index_for_length_m : i + 1])\n",
    "\n",
    "def process_chunk(x, M, N):\n",
    "    return Counter(get_seq(x, M)).most_common(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_chunk(tag_collection_by_rule, M=2, N=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for number in range(49):\n",
    "#     print(number)\n",
    "#     j = process_chunk(tag_collection_by_rule, M=number, N=50)\n",
    "#     for k in j:\n",
    "#         print(k[0])\n",
    "#         print(k[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DictBasedTrie(object):\n",
    "\n",
    "    # the constructor creates an empty trie\n",
    "    def __init__(self):\n",
    "        self.data = {}\n",
    "\n",
    "    # add a word by adding each character on different levels\n",
    "    def Add(self, list_of_strings):\n",
    "\n",
    "        # k is the current level\n",
    "        k = self.data\n",
    "        \n",
    "        # for each character in the word\n",
    "        for c in list_of_strings:\n",
    "\n",
    "            # if we've seen this character on this level before, use it\n",
    "            try:\n",
    "                k = k[c]\n",
    "            except KeyError:\n",
    "                # we haven't => create a new sub dict\n",
    "                k[c] = {}\n",
    "                k = k[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! -*- Encoding: Latin-1 -*-\n",
    "import pprint\n",
    "def selftest():\n",
    "    t = DictBasedTrie()\n",
    "    for word in tag_collection_by_rule:\n",
    "        t.Add(word)\n",
    "        \n",
    "#     pprint.pprint(t.data)\n",
    "    return t.data\n",
    "if __name__ == \"__main__\":\n",
    "    selftest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join(d, pref=[]):\n",
    "    if end in d:\n",
    "        yield [' '.join(pref)] if pref else []\n",
    "    for k, v in d.items():\n",
    "#         print(k)\n",
    "#         print(v)\n",
    "        if len(v) == 1:\n",
    "            for x in join(v, pref + [k]): # add node to prefix\n",
    "                yield x                   # yield next segment\n",
    "        else:\n",
    "            for x in join(v, []):         # reset prefix\n",
    "                yield [' '.join(pref + [k])] + x # yield node + prefix and next\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = {}\n",
    "end = \"END\"\n",
    "for lst in tag_collection_by_rule:\n",
    "    d = tree\n",
    "    for x in lst:\n",
    "        d = d.setdefault(x, {})\n",
    "    d[end] = {}\n",
    "len(tree.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in join(tree):\n",
    "#     print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pydot\n",
    "\n",
    "# menu = {'dinner':\n",
    "#             {'chicken':'good',\n",
    "#              'beef':'average',\n",
    "#              'vegetarian':{\n",
    "#                    'tofu':'good',\n",
    "#                    'salad':{\n",
    "#                             'caeser':'bad',\n",
    "#                             'italian':'average'}\n",
    "#                    },\n",
    "#              'pork':'bad'}\n",
    "#         }\n",
    "\n",
    "# def draw(parent_name, child_name):\n",
    "#     edge = pydot.Edge(parent_name, child_name)\n",
    "#     graph.add_edge(edge)\n",
    "\n",
    "# def visit(node, parent=None):\n",
    "#     for k,v in node.items():\n",
    "#         if isinstance(v, dict):\n",
    "#             # We start with the root node whose parent is None\n",
    "#             # we don't want to graph the None node\n",
    "#             if parent:\n",
    "#                 draw(parent, k[0])\n",
    "#             visit(v, k[0])\n",
    "#         else:\n",
    "#             draw(parent, k[0])\n",
    "#             # drawing the label using a distinct name\n",
    "#             draw(k[0], k[0]+'_'+v)\n",
    "\n",
    "# graph = pydot.Dot(graph_type='graph')\n",
    "# f = selftest()\n",
    "# print(f)\n",
    "# visit(selftest())\n",
    "# graph.write_png('tree_graph.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lcsubstring_length(a, b):\n",
    "    table = [[0] * (len(b) + 1) for _ in xrange(len(a) + 1)]\n",
    "    l = 0\n",
    "    for i, ca in enumerate(a, 1):\n",
    "        for j, cb in enumerate(b, 1):\n",
    "            if ca == cb:\n",
    "                table[i][j] = table[i - 1][j - 1] + 1\n",
    "                if table[i][j] > l:\n",
    "                    l = table[i][j]\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from difflib import SequenceMatcher\n",
    "def func(x, y):\n",
    "    s = SequenceMatcher(None, x, y)\n",
    "    return s.find_longest_match(0, len(x), 0, len(y)).size\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
