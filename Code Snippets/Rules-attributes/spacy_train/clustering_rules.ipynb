{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import nltk\n",
    "import spacy\n",
    "from operator import itemgetter\n",
    "import itertools\n",
    "import random\n",
    "import Levenshtein\n",
    "import difflib\n",
    "import pickle\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "def depickle(file):\n",
    "    file = open(file, 'rb')\n",
    "    data = pickle.load(file)\n",
    "    file.close()\n",
    "    return data\n",
    "def load_json_file(file):\n",
    "    with open(file) as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.In order to remove any difficulties in respect of the application or interpretation of these regulations, the Board may issue clarifications or guidelines in the form of circulars.\n",
      "27.In order to remove any difficulties in the interpretation or application of the provisions of these regulations, the Board may issue clarifications or guidelines from time to time.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7486338797814208"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def similarity_between_rules(r, verbose =True):\n",
    "    s1 = r[0]\n",
    "    s2 = r[1]\n",
    "#     s1 = tag_words(rule1)\n",
    "#     s2 = tag_words(rule2)\n",
    "    s = difflib.SequenceMatcher(None, s1, s2)\n",
    "    verbose_list = []\n",
    "    if verbose:\n",
    "        for tag, i1, i2, j1, j2 in s.get_opcodes():\n",
    "            verbose_list.append((tag, s1[i1:i2], s2[j1:j2]))\n",
    "    return (verbose_list,s.ratio())\n",
    "\n",
    "def get_score_only(s1,s2,verbose=False):\n",
    "    s = difflib.SequenceMatcher(None, s1, s2)\n",
    "    verbose_list = []\n",
    "    if verbose:\n",
    "        for tag, i1, i2, j1, j2 in s.get_opcodes():\n",
    "            verbose_list.append((tag, s1[i1:i2], s2[j1:j2]))\n",
    "    return s.ratio()\n",
    "\n",
    "\n",
    "def rSubset(arr, r): \n",
    "    return list(itertools.combinations(arr, r))\n",
    "\n",
    "def tag_words(s):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    output_dir = 'my_mixed_random/'\n",
    "    nlp2 = spacy.load(output_dir)\n",
    "    ent_list = []\n",
    "    doc2 = nlp2(s)\n",
    "    for ent in doc2.ents:\n",
    "        ent_list.append({'label':ent.label_, 'start':ent.start, 'end':ent.end})\n",
    "    doc = nlp(s)\n",
    "    for token in doc:\n",
    "        ent_list.append({'label':token.pos_, 'start':token.idx, 'end': token.idx+len(token.text)})\n",
    "    ent_list = sorted(ent_list,\n",
    "                          key=lambda k: (k['start']))\n",
    "    sign_list = []\n",
    "    for tag in ent_list:\n",
    "        sign_list.append(tag['label'])\n",
    "    return sign_list\n",
    "\n",
    "r1 = '43.In order to remove any difficulties in respect of the application or interpretation of these regulations, the Board may issue clarifications or guidelines in the form of circulars.'\n",
    "r2 = '27.In order to remove any difficulties in the interpretation or application of the provisions of these regulations, the Board may issue clarifications or guidelines from time to time.'\n",
    "print(r1)\n",
    "print(r2)\n",
    "similarity_between_rules((r1,r2))[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_rules' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-93713467b327>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_rules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'all_rules' is not defined"
     ]
    }
   ],
   "source": [
    "len(all_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6012"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_rules = load_json_file('ujwal_annotated_all_json.json')\n",
    "rule_tag_dict = {}\n",
    "rule_without_pos = {}\n",
    "#to store tags rule wise.\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "output_dir = 'best_trained/'\n",
    "nlp2 = spacy.load(output_dir)\n",
    "count = 0\n",
    "for rule in all_rules.values():\n",
    "    ent_list = []\n",
    "    count += 1\n",
    "    for labs,text in zip(rule['ent_labels'],rule['ents']):\n",
    "        ent_list.append({'start':rule['text'].find(text), 'end':rule['text'].find(text)+len(text), 'label':labs})\n",
    "    doc2 = nlp2(rule['text'])\n",
    "    for ent in doc2.ents:\n",
    "        ent_list.append({'label':ent.label_, 'start':ent.start_char, 'end':ent.end_char})\n",
    "    doc = nlp(rule['text'])\n",
    "    for token in doc:\n",
    "        if token.pos_  in [\"VERB\", \"ADJ\"]:\n",
    "            ent_list.append({'label':token.pos_, 'start':token.idx, 'end': token.idx+len(token.text)})\n",
    "    #sorting spans.\n",
    "    ent_list = sorted(ent_list,\n",
    "                          key=lambda k: (k['start']))\n",
    "    rule_tag_dict[rule['text']] = ent_list\n",
    "rule_tag_dict\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(rule_tag_dict,'pickled files/all_rules_all_annotations.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_dict = {}\n",
    "# for rule in rule_tag_dict:\n",
    "#     if len(rule)>30 and ('Page' not in rule) and ('Amendment' not in rule) and ('.....' not in rule) and('…………' not in rule):\n",
    "#         sentence_dict[rule] = [k['label'] for k in rule_tag_dict[rule]]\n",
    "#         print(rule)\n",
    "# print(len(sentence_dict))\n",
    "# # print(sentence_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_similarity_dict = {}\n",
    "for s in sentence_dict:\n",
    "    for l in sentence_dict:\n",
    "        if s!=l:\n",
    "#             if ((s,l) not in rule_similarity_dict.keys()) or ((l,s) not in rule_similarity_dict.keys()):\n",
    "            rule_similarity_dict[(s,l)] = get_score_only(sentence_dict[s],sentence_dict[l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_object(rule_similarity_dict, 'pickled rules/similarity_scores_btw_all_rules.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import networkx as nx\n",
    "#pass a dictionary consisting of rules and tag signs as values.\n",
    "def clustering_sentences(sentence_dict, threshold):\n",
    "    clusters = []\n",
    "    remaining_rules = list(sentence_dict.keys())\n",
    "    while(len(remaining_rules)!=0):\n",
    "        seed_rule = random.choice(remaining_rules)\n",
    "        #initial values.\n",
    "        cluster = [seed_rule]\n",
    "#         print(seed_rule+\"8888888888\")\n",
    "#         remaining_rules = list(sentence_dict.keys())\n",
    "        remaining_rules.remove(seed_rule)\n",
    "        #make a cluster grow its optimal size.\n",
    "        #keep doing until size of cluster remains unchanged.\n",
    "        old_size = 1\n",
    "        new_size = 100\n",
    "        while (new_size>old_size):\n",
    "            old_size = len(cluster)\n",
    "#             print(local_similarity)\n",
    "            local_similarity = {}\n",
    "            for k in remaining_rules:\n",
    "                for r in cluster:\n",
    "                    try:\n",
    "                        local_similarity[(k,r)] = rule_similarity_dict[(k,r)]\n",
    "                    except:\n",
    "                        local_similarity[(k,r)] = get_score_only(sentence_dict[k],sentence_dict[r])\n",
    "#                     else:\n",
    "#                         local_similarity[(k,r)] = rule_similarity_dict[(k,r)]\n",
    "            add_to_cluster = {k[0]:v for (k,v) in local_similarity.items() if v >= threshold}\n",
    "            remaining = {k[0]:v for (k,v) in local_similarity.items() if v < threshold}\n",
    "            cluster.extend(add_to_cluster.keys())\n",
    "            cluster = list(set(cluster))\n",
    "            remaining_rules = remaining.keys()\n",
    "            remaining_rules = list(set(remaining_rules))\n",
    "            new_size = len(cluster)\n",
    "            new_clusters = []\n",
    "            #updating clusters midway for efficient transactions.\n",
    "            for clu in clusters:\n",
    "                if clu!= cluster:\n",
    "                    if set(clu).intersection(set(cluster)):\n",
    "                        cluster.extend(clu)\n",
    "#                         new_clusters.append(cluster)\n",
    "                    else:\n",
    "                        new_clusters.append(clu)\n",
    "            clusters = new_clusters\n",
    "                        \n",
    "#             print(new_size)\n",
    "        print(cluster)\n",
    "        clusters.append(cluster)\n",
    "    #merging clusters if they consist of the same elements.\n",
    "    print(clusters)\n",
    "    L = clusters\n",
    "    G = nx.Graph()    \n",
    "    G.add_nodes_from(sum(L, []))\n",
    "    q = [[(s[i],s[i+1]) for i in range(len(s)-1)] for s in L]\n",
    "    for i in q:\n",
    "        G.add_edges_from(i)\n",
    "    clusters = [list(i) for i in nx.connected_components(G)]\n",
    "#     sign_clusters = []\n",
    "#     for cluster in clusters:\n",
    "#         sign_rule_dict = {}\n",
    "#         for rule in cluster:\n",
    "#             sign_rule_dict[rule] = sentence_dict[rule]\n",
    "    return clusters\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster070= clustering_sentences(sentence_dict, 0.70)\n",
    "# save_object(cluster070, 'clustering_results/cluster070')\n",
    "# cluster075= clustering_sentences(sentence_dict, 0.75)\n",
    "# save_object(cluster075, 'clustering_results/cluster075')\n",
    "# cluster065= clustering_sentences(sentence_dict, 0.65)\n",
    "# save_object(cluster065, 'clustering_results/cluster065')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2430"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cluster080)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
